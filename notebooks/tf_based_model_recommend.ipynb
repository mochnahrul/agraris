{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Crop Recommendation\n",
    "\n",
    "## Summary & How to Use\n",
    "\n",
    "Agraris is a model to classify the land based on the user's input to get the best crop for the land. By using this model, we can get the best kind of crop to be cultivated in the user's respective land to maximize the productivity of the land."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make the model?\n",
    "\n",
    "### Data Pre-processing\n",
    "\n",
    "1. Load the dataset from the dataset folder from **\"dataset/Crop_recommendation.csv\"** using pandas.\n",
    "\n",
    "2. Get the description of the data statistically to find the overall value of the data by using **describe()**.\n",
    "\n",
    "3. **Encode** the labels to integer value (as a string is a little bit complicated to be used as it is). After it is encoded as integer, a **JSON** file is made to **keep track of the encoded labels**. So, the **dictionary** of **key-value of string-integer pair** is stored in a file named **\"dataset/labelEncoder_dict.json\"**.\n",
    "\n",
    "4. **Scale** the data (as the data's range are too various, and it will be a hard job for the system to directly process the data into the model) using **MinMaxScaler of Sci-Kit Learn**. The scaled data are applied to the dataframe for visualization purpose.\n",
    "\n",
    "   **Note: The scaled data are only the features. Labels are NOT scaled.**\n",
    "\n",
    "   \n",
    "\n",
    "### Data Preparation for Modelling\n",
    "\n",
    "5. Separate/select the label, and the feature. As the label is located in the last index (the 7th index), and put the other column into the features columns.\n",
    "6. Cast the 22 labels into a simplified 22 columns of binary number, as binary numbers are way easier for the system to process rather than the number as it is. To do this, use the Keras' utility, the tf.keras.utils.to_categorical([array]).\n",
    "7. Split and shuffle the feature and the label (X and y) into 2 different usage. In this scoop, the feature is divided as the train set and test set with the proportion of 90% train set, and the 10% as the test set.\n",
    "\n",
    "\n",
    "\n",
    "### Modelling Process\n",
    "\n",
    "8. Define the model. In this scoop, the model uses 4 layers of Keras' Dense layer. The activation function used in the model is ReLu for the first 3 layers, as the ReLU activation function is one of the best activation function.\n",
    "\n",
    "   The last activation function is different (softmax) is used to do multi-class classification.\n",
    "\n",
    "9. Define the callbacks that are going to be used. In this case, there are two callbacks that are used in this model. The first callback is the Reminder callback, which is used to get the notification that the accuracy has reached the threshold of the accuracy (in this scoop is 0.99). This callback is used to flag the \"good\" model, and differ it from the others.\n",
    "\n",
    "10. Define the second callback (checkpointCB). This checkpoint is used to save the best model that this iteration has ever had. The model is stored at the model folder. This callback is used to get the best model for the prediction later, or just to simply save the model as a .h5 file to be able to be reused.\n",
    "\n",
    "11. Compile the model. The loss used in this optimization is the categorical cross-entropy (this loss is optimized for multi-class classification). To fit this model better, the Root Mean Square Propagation (RMS-Prop) with the accuracy as the metric.\n",
    "\n",
    "12. Fit the model with the epoch of 200, and the steps per epoch is 50.\n",
    "\n",
    "    \n",
    "\n",
    "### Evaluation\n",
    "\n",
    "13. Plot the accuracy of the model in the epoch. The plot is for accuracy of both the train set and the test set. After plotting, check the graph. Does the accuracy continue to increase or keep decreasing? If it continues to increase, then the model is well-fit.\n",
    "14. Plot the loss of the model in the epoch. As the opposite of accuracy, the plot is for loss of both the train set and the test set should decrease in each epoch. After plotting, check the graph. Does the loss continue to decrease or keep increasing? If it continues to decrease, then the model is well-fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "As the data from the dataset is the raw data, we need to process the data first before it is able to be used well. To get the best model, we need to supply it with the best processed data, not a raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries\n",
    "Load the libraries needed by using import. The libraries needed:\n",
    "\n",
    "- Pandas\n",
    "- NumPy\n",
    "- Sci-Kit Learn\n",
    "- Tensorflow\n",
    "- JSON\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Load the dataset from the dataset folder from **\"dataset/Crop_recommendation.csv\"** using pandas.\n",
    "\n",
    "This dataset is the main dataframe that will be used in this process from the pre-processing step, into the modelling step. The result of the raw dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>20.879744</td>\n",
       "      <td>82.002744</td>\n",
       "      <td>6.502985</td>\n",
       "      <td>202.935536</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>21.770462</td>\n",
       "      <td>80.319644</td>\n",
       "      <td>7.038096</td>\n",
       "      <td>226.655537</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>23.004459</td>\n",
       "      <td>82.320763</td>\n",
       "      <td>7.840207</td>\n",
       "      <td>263.964248</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>26.491096</td>\n",
       "      <td>80.158363</td>\n",
       "      <td>6.980401</td>\n",
       "      <td>242.864034</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>20.130175</td>\n",
       "      <td>81.604873</td>\n",
       "      <td>7.628473</td>\n",
       "      <td>262.717340</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>107</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>26.774637</td>\n",
       "      <td>66.413269</td>\n",
       "      <td>6.780064</td>\n",
       "      <td>177.774507</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>99</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>27.417112</td>\n",
       "      <td>56.636362</td>\n",
       "      <td>6.086922</td>\n",
       "      <td>127.924610</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>118</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>24.131797</td>\n",
       "      <td>67.225123</td>\n",
       "      <td>6.362608</td>\n",
       "      <td>173.322839</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>117</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>26.272418</td>\n",
       "      <td>52.127394</td>\n",
       "      <td>6.758793</td>\n",
       "      <td>127.175293</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>104</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>23.603016</td>\n",
       "      <td>60.396475</td>\n",
       "      <td>6.779833</td>\n",
       "      <td>140.937041</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        N   P   K  temperature   humidity        ph    rainfall   label\n",
       "0      90  42  43    20.879744  82.002744  6.502985  202.935536    rice\n",
       "1      85  58  41    21.770462  80.319644  7.038096  226.655537    rice\n",
       "2      60  55  44    23.004459  82.320763  7.840207  263.964248    rice\n",
       "3      74  35  40    26.491096  80.158363  6.980401  242.864034    rice\n",
       "4      78  42  42    20.130175  81.604873  7.628473  262.717340    rice\n",
       "...   ...  ..  ..          ...        ...       ...         ...     ...\n",
       "2195  107  34  32    26.774637  66.413269  6.780064  177.774507  coffee\n",
       "2196   99  15  27    27.417112  56.636362  6.086922  127.924610  coffee\n",
       "2197  118  33  30    24.131797  67.225123  6.362608  173.322839  coffee\n",
       "2198  117  32  34    26.272418  52.127394  6.758793  127.175293  coffee\n",
       "2199  104  18  30    23.603016  60.396475  6.779833  140.937041  coffee\n",
       "\n",
       "[2200 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "\n",
    "file_name = \"dataset/Crop_recommendation.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Description of Dataset\n",
    "\n",
    "Get the description of the data statistically to find the overall value of the data by using **describe()**.\n",
    "\n",
    "By getting this information, we can get the overview of the dataset, thus helping us in preprocessing the data (determine the scaling, and the encoding process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.551818</td>\n",
       "      <td>53.362727</td>\n",
       "      <td>48.149091</td>\n",
       "      <td>25.616244</td>\n",
       "      <td>71.481779</td>\n",
       "      <td>6.469480</td>\n",
       "      <td>103.463655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36.917334</td>\n",
       "      <td>32.985883</td>\n",
       "      <td>50.647931</td>\n",
       "      <td>5.063749</td>\n",
       "      <td>22.263812</td>\n",
       "      <td>0.773938</td>\n",
       "      <td>54.958389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.825675</td>\n",
       "      <td>14.258040</td>\n",
       "      <td>3.504752</td>\n",
       "      <td>20.211267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.769375</td>\n",
       "      <td>60.261953</td>\n",
       "      <td>5.971693</td>\n",
       "      <td>64.551686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>25.598693</td>\n",
       "      <td>80.473146</td>\n",
       "      <td>6.425045</td>\n",
       "      <td>94.867624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>84.250000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>28.561654</td>\n",
       "      <td>89.948771</td>\n",
       "      <td>6.923643</td>\n",
       "      <td>124.267508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>140.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>43.675493</td>\n",
       "      <td>99.981876</td>\n",
       "      <td>9.935091</td>\n",
       "      <td>298.560117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 N            P            K  temperature     humidity  \\\n",
       "count  2200.000000  2200.000000  2200.000000  2200.000000  2200.000000   \n",
       "mean     50.551818    53.362727    48.149091    25.616244    71.481779   \n",
       "std      36.917334    32.985883    50.647931     5.063749    22.263812   \n",
       "min       0.000000     5.000000     5.000000     8.825675    14.258040   \n",
       "25%      21.000000    28.000000    20.000000    22.769375    60.261953   \n",
       "50%      37.000000    51.000000    32.000000    25.598693    80.473146   \n",
       "75%      84.250000    68.000000    49.000000    28.561654    89.948771   \n",
       "max     140.000000   145.000000   205.000000    43.675493    99.981876   \n",
       "\n",
       "                ph     rainfall  \n",
       "count  2200.000000  2200.000000  \n",
       "mean      6.469480   103.463655  \n",
       "std       0.773938    54.958389  \n",
       "min       3.504752    20.211267  \n",
       "25%       5.971693    64.551686  \n",
       "50%       6.425045    94.867624  \n",
       "75%       6.923643   124.267508  \n",
       "max       9.935091   298.560117  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the label column\n",
    "\n",
    "**Encode** the labels to integer value (as a string is a little bit complicated to be used as it is). After it is encoded as integer, a **JSON** file is made to **keep track of the encoded labels**. So, the **dictionary** of **key-value of string-integer pair** is stored in a file named **\"dataset/labelEncoder_dict.json\"**.\n",
    "\n",
    "This process is used to make the data processing by the system better, and faster, as numbers are easier to process than string. After the label is encoded, a JSON file is made to keep track of the encoded string-integer pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Label</th>\n",
       "      <th>Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackgram</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chickpea</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coconut</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coffee</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cotton</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grapes</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jute</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kidneybeans</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lentil</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>maize</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mango</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mothbeans</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mungbean</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>muskmelon</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>orange</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>papaya</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pigeonpeas</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pomegranate</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rice</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>watermelon</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Label  Encoded\n",
       "0           apple        0\n",
       "1          banana        1\n",
       "2       blackgram        2\n",
       "3        chickpea        3\n",
       "4         coconut        4\n",
       "5          coffee        5\n",
       "6          cotton        6\n",
       "7          grapes        7\n",
       "8            jute        8\n",
       "9     kidneybeans        9\n",
       "10         lentil       10\n",
       "11          maize       11\n",
       "12          mango       12\n",
       "13      mothbeans       13\n",
       "14       mungbean       14\n",
       "15      muskmelon       15\n",
       "16         orange       16\n",
       "17         papaya       17\n",
       "18     pigeonpeas       18\n",
       "19    pomegranate       19\n",
       "20           rice       20\n",
       "21     watermelon       21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode labels to int, and parse to json\n",
    "labelEncoder = LabelEncoder()\n",
    "\n",
    "labelEncoder.fit(df[\"label\"])\n",
    "labelEncoder_key_value = dict(zip(labelEncoder.classes_, labelEncoder.transform(labelEncoder.classes_)))\n",
    "\n",
    "keys_values = labelEncoder_key_value.items()\n",
    "output_key_val = {str(key): str(value) for key, value in keys_values}\n",
    "\n",
    "with open('dataset/labelEncoder_dict.json', 'w') as output_file:\n",
    "    json.dump(output_key_val, output_file, indent=4)\n",
    "\n",
    "pd.DataFrame(labelEncoder_key_value.items(), columns=['Original Label', 'Encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>20.879744</td>\n",
       "      <td>82.002744</td>\n",
       "      <td>6.502985</td>\n",
       "      <td>202.935536</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>21.770462</td>\n",
       "      <td>80.319644</td>\n",
       "      <td>7.038096</td>\n",
       "      <td>226.655537</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>23.004459</td>\n",
       "      <td>82.320763</td>\n",
       "      <td>7.840207</td>\n",
       "      <td>263.964248</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>26.491096</td>\n",
       "      <td>80.158363</td>\n",
       "      <td>6.980401</td>\n",
       "      <td>242.864034</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>20.130175</td>\n",
       "      <td>81.604873</td>\n",
       "      <td>7.628473</td>\n",
       "      <td>262.717340</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>107</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>26.774637</td>\n",
       "      <td>66.413269</td>\n",
       "      <td>6.780064</td>\n",
       "      <td>177.774507</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>99</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>27.417112</td>\n",
       "      <td>56.636362</td>\n",
       "      <td>6.086922</td>\n",
       "      <td>127.924610</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>118</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>24.131797</td>\n",
       "      <td>67.225123</td>\n",
       "      <td>6.362608</td>\n",
       "      <td>173.322839</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>117</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>26.272418</td>\n",
       "      <td>52.127394</td>\n",
       "      <td>6.758793</td>\n",
       "      <td>127.175293</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>104</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>23.603016</td>\n",
       "      <td>60.396475</td>\n",
       "      <td>6.779833</td>\n",
       "      <td>140.937041</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        N   P   K  temperature   humidity        ph    rainfall  label\n",
       "0      90  42  43    20.879744  82.002744  6.502985  202.935536     20\n",
       "1      85  58  41    21.770462  80.319644  7.038096  226.655537     20\n",
       "2      60  55  44    23.004459  82.320763  7.840207  263.964248     20\n",
       "3      74  35  40    26.491096  80.158363  6.980401  242.864034     20\n",
       "4      78  42  42    20.130175  81.604873  7.628473  262.717340     20\n",
       "...   ...  ..  ..          ...        ...       ...         ...    ...\n",
       "2195  107  34  32    26.774637  66.413269  6.780064  177.774507      5\n",
       "2196   99  15  27    27.417112  56.636362  6.086922  127.924610      5\n",
       "2197  118  33  30    24.131797  67.225123  6.362608  173.322839      5\n",
       "2198  117  32  34    26.272418  52.127394  6.758793  127.175293      5\n",
       "2199  104  18  30    23.603016  60.396475  6.779833  140.937041      5\n",
       "\n",
       "[2200 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply to dataset\n",
    "\n",
    "df[\"label\"] = labelEncoder.transform(df[\"label\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data using MinMax Scaler\n",
    "\n",
    "**Scale** the data (as the data's range are too various, and it will be a hard job for the system to directly process the data into the model) using **MinMaxScaler of Sci-Kit Learn**. The scaled data are applied to the dataframe for visualization purpose.\n",
    "\n",
    "**Note: The scaled data are only the features. Labels are NOT scaled.**\n",
    "\n",
    "The data is scaled to process this data better. The data is hard to process without the scaling process as the range is wide. Therefore, the scaling process is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling to reduce range\n",
    "\n",
    "minmaxScaler = MinMaxScaler()\n",
    "minmaxScaler.fit(df.iloc[:, 0:-1])\n",
    "df.iloc[:, 0:-1] = minmaxScaler.transform(df.iloc[:, 0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.345886</td>\n",
       "      <td>0.790267</td>\n",
       "      <td>0.466264</td>\n",
       "      <td>0.656458</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.378571</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.371445</td>\n",
       "      <td>0.770633</td>\n",
       "      <td>0.549480</td>\n",
       "      <td>0.741675</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.406854</td>\n",
       "      <td>0.793977</td>\n",
       "      <td>0.674219</td>\n",
       "      <td>0.875710</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.506901</td>\n",
       "      <td>0.768751</td>\n",
       "      <td>0.540508</td>\n",
       "      <td>0.799905</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.324378</td>\n",
       "      <td>0.785626</td>\n",
       "      <td>0.641291</td>\n",
       "      <td>0.871231</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>0.764286</td>\n",
       "      <td>0.207143</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.515037</td>\n",
       "      <td>0.608410</td>\n",
       "      <td>0.509353</td>\n",
       "      <td>0.566064</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.533473</td>\n",
       "      <td>0.494359</td>\n",
       "      <td>0.401561</td>\n",
       "      <td>0.386972</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.439202</td>\n",
       "      <td>0.617880</td>\n",
       "      <td>0.444433</td>\n",
       "      <td>0.550071</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>0.835714</td>\n",
       "      <td>0.192857</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.500627</td>\n",
       "      <td>0.441760</td>\n",
       "      <td>0.506045</td>\n",
       "      <td>0.384280</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.424029</td>\n",
       "      <td>0.538222</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.433721</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             N         P      K  temperature  humidity        ph  rainfall  \\\n",
       "0     0.642857  0.264286  0.190     0.345886  0.790267  0.466264  0.656458   \n",
       "1     0.607143  0.378571  0.180     0.371445  0.770633  0.549480  0.741675   \n",
       "2     0.428571  0.357143  0.195     0.406854  0.793977  0.674219  0.875710   \n",
       "3     0.528571  0.214286  0.175     0.506901  0.768751  0.540508  0.799905   \n",
       "4     0.557143  0.264286  0.185     0.324378  0.785626  0.641291  0.871231   \n",
       "...        ...       ...    ...          ...       ...       ...       ...   \n",
       "2195  0.764286  0.207143  0.135     0.515037  0.608410  0.509353  0.566064   \n",
       "2196  0.707143  0.071429  0.110     0.533473  0.494359  0.401561  0.386972   \n",
       "2197  0.842857  0.200000  0.125     0.439202  0.617880  0.444433  0.550071   \n",
       "2198  0.835714  0.192857  0.145     0.500627  0.441760  0.506045  0.384280   \n",
       "2199  0.742857  0.092857  0.125     0.424029  0.538222  0.509317  0.433721   \n",
       "\n",
       "      label  \n",
       "0        20  \n",
       "1        20  \n",
       "2        20  \n",
       "3        20  \n",
       "4        20  \n",
       "...     ...  \n",
       "2195      5  \n",
       "2196      5  \n",
       "2197      5  \n",
       "2198      5  \n",
       "2199      5  \n",
       "\n",
       "[2200 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df before modelling\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Modelling\n",
    "\n",
    "The data is already \"cleansed\" and \"chopped\" by doing the pre-processing process. What's left is to prepare it for the modelling process. In this part, we are going to prepare it to be ready to be used by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature & Label Selection\n",
    "\n",
    "Separate/select the label, and the feature. As the label is located in the last index (the 7th index), and put the other column into the features columns.\n",
    "\n",
    "The X and y (feature and label) has not been defined yet. The data is still in the model of dataframe, and it needs to be separated as feature, and label.\n",
    "The label is the data from the last column, while the feature(s) are the rest of the data other than label.\n",
    "\n",
    "This is why the separation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature x label selection\n",
    "\n",
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the label using one-hot Encoding\n",
    "\n",
    "Encode the 22 labels into a simplified 22 columns of binary number, as binary numbers are way easier for the system to process rather than the number as it is. To do this, use the Keras' utility, the tf.keras.utils.to_categorical([array]).\n",
    "\n",
    "By using this syntax, the data are able to be processed way better than using only one column. By encoding the data, the system can process it more precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding for label\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split & Shuffle to Train Set and Test Set\n",
    "\n",
    "Split and shuffle the feature and the label (X and y) into 2 different usage. In this scoop, the feature is divided as the train set and test set with the proportion of 90% train set, and the 10% as the test set.\n",
    "\n",
    "This split is performed to be able to use 1 dataset as the train set, as it is a test set too, so that one dataset is enough for the development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Process\n",
    "\n",
    "The modelling process is the main topic of this document, as the modelling is the part where we make a new model based on the need. These are the steps in the model which will be used in the model.\n",
    "\n",
    "1. Define the model.\n",
    "    - In this scoop, the model uses 4 layers of Keras' Dense layer. The activation function used in the model is ReLu for the first 3 layers, as the ReLU activation function is one of the best activation function.\n",
    "    - The last activation function is different (softmax) is used to do multi-class classification.\n",
    "2. Define the callbacks that are going to be used.\n",
    "    - In this case, there are two callbacks that are used in this model. The first callback is the Reminder callback, which is used to get the notification that the accuracy has reached the threshold of the accuracy (in this scoop is 0.99). This callback is used to flag the \"good\" model, and differ it from the others.\n",
    "\n",
    "3. Define the second callback (checkpointCB).\n",
    "    - This checkpoint is used to save the best model that this iteration has ever had. The model is stored at the model folder. This callback is used to get the best model for the prediction later, or just to simply save the model as a .h5 file to be able to be reused.\n",
    "\n",
    "4. Compile the model.\n",
    "    - The loss used in this optimization is the categorical cross-entropy (this loss is optimized for multi-class classification). To fit this model better, the Root Mean Square Propagation (RMS-Prop) with the accuracy as the metric.\n",
    "\n",
    "5. Fit the model.\n",
    "    - Fit the complied model with the epoch of 200, and the steps per epoch is 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50/50 [==============================] - 1s 6ms/step - loss: 2.4963 - accuracy: 0.3763 - val_loss: 1.8731 - val_accuracy: 0.5091\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.3580 - accuracy: 0.6525 - val_loss: 1.0498 - val_accuracy: 0.7364\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8186 - accuracy: 0.7606 - val_loss: 0.7665 - val_accuracy: 0.8000\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5913 - accuracy: 0.8338 - val_loss: 0.5902 - val_accuracy: 0.8273\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.8747 - val_loss: 0.5721 - val_accuracy: 0.7955\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8803 - val_loss: 0.4127 - val_accuracy: 0.8636\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3262 - accuracy: 0.9066 - val_loss: 0.3956 - val_accuracy: 0.8591\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.9121 - val_loss: 0.2980 - val_accuracy: 0.8955\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2534 - accuracy: 0.9268 - val_loss: 0.2896 - val_accuracy: 0.8773\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2238 - accuracy: 0.9328 - val_loss: 0.3139 - val_accuracy: 0.8682\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.9419 - val_loss: 0.2955 - val_accuracy: 0.8864\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9394 - val_loss: 0.2375 - val_accuracy: 0.9182\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1679 - accuracy: 0.9444 - val_loss: 0.2641 - val_accuracy: 0.9136\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9500 - val_loss: 0.2365 - val_accuracy: 0.8864\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9465 - val_loss: 0.2047 - val_accuracy: 0.9091\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.9566 - val_loss: 0.1705 - val_accuracy: 0.9318\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.9561 - val_loss: 0.1911 - val_accuracy: 0.9182\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1312 - accuracy: 0.9556 - val_loss: 0.1750 - val_accuracy: 0.9273\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9601 - val_loss: 0.2468 - val_accuracy: 0.8909\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.9636 - val_loss: 0.1464 - val_accuracy: 0.9227\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9636 - val_loss: 0.1487 - val_accuracy: 0.9455\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 0.9641 - val_loss: 0.1378 - val_accuracy: 0.9364\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9631 - val_loss: 0.1437 - val_accuracy: 0.9455\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1062 - accuracy: 0.9677 - val_loss: 0.2757 - val_accuracy: 0.9045\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9636 - val_loss: 0.1426 - val_accuracy: 0.9545\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0945 - accuracy: 0.9641 - val_loss: 0.1109 - val_accuracy: 0.9545\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0877 - accuracy: 0.9677 - val_loss: 0.1235 - val_accuracy: 0.9500\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0869 - accuracy: 0.9702 - val_loss: 0.1287 - val_accuracy: 0.9455\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0891 - accuracy: 0.9672 - val_loss: 0.0966 - val_accuracy: 0.9682\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0868 - accuracy: 0.9687 - val_loss: 0.1450 - val_accuracy: 0.9409\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9712 - val_loss: 0.1016 - val_accuracy: 0.9682\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9717 - val_loss: 0.0910 - val_accuracy: 0.9682\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9758 - val_loss: 0.1058 - val_accuracy: 0.9545\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9742 - val_loss: 0.1015 - val_accuracy: 0.9727\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9753 - val_loss: 0.1190 - val_accuracy: 0.9500\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0671 - accuracy: 0.9727 - val_loss: 0.0892 - val_accuracy: 0.9682\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0674 - accuracy: 0.9722 - val_loss: 0.1409 - val_accuracy: 0.9545\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9737 - val_loss: 0.1477 - val_accuracy: 0.9545\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9753 - val_loss: 0.1432 - val_accuracy: 0.9545\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9778 - val_loss: 0.1239 - val_accuracy: 0.9591\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9768 - val_loss: 0.0751 - val_accuracy: 0.9591\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9773 - val_loss: 0.0670 - val_accuracy: 0.9682\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9737 - val_loss: 0.1075 - val_accuracy: 0.9591\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9747 - val_loss: 0.0624 - val_accuracy: 0.9773\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9778 - val_loss: 0.1130 - val_accuracy: 0.9500\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9753 - val_loss: 0.0815 - val_accuracy: 0.9773\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9843 - val_loss: 0.2090 - val_accuracy: 0.9136\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9783 - val_loss: 0.0688 - val_accuracy: 0.9727\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9808 - val_loss: 0.1406 - val_accuracy: 0.9545\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9808 - val_loss: 0.0785 - val_accuracy: 0.9773\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.0999 - val_accuracy: 0.9591\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9773 - val_loss: 0.1044 - val_accuracy: 0.9500\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9793 - val_loss: 0.1066 - val_accuracy: 0.9591\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9818 - val_loss: 0.0691 - val_accuracy: 0.9727\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9848 - val_loss: 0.0690 - val_accuracy: 0.9818\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 0.9813 - val_loss: 0.0486 - val_accuracy: 0.9773\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9818 - val_loss: 0.0411 - val_accuracy: 0.9864\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0446 - accuracy: 0.9828 - val_loss: 0.0568 - val_accuracy: 0.9591\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9823 - val_loss: 0.0399 - val_accuracy: 0.9818\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9813 - val_loss: 0.0654 - val_accuracy: 0.9636\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9823 - val_loss: 0.1009 - val_accuracy: 0.9636\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9848 - val_loss: 0.0489 - val_accuracy: 0.9818\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 0.9798 - val_loss: 0.1066 - val_accuracy: 0.9545\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9818 - val_loss: 0.1583 - val_accuracy: 0.9500\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9838 - val_loss: 0.0532 - val_accuracy: 0.9727\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0440 - accuracy: 0.9803 - val_loss: 0.1391 - val_accuracy: 0.9455\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0419 - accuracy: 0.9843 - val_loss: 0.0852 - val_accuracy: 0.9545\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9874 - val_loss: 0.0811 - val_accuracy: 0.9682\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0361 - accuracy: 0.9859 - val_loss: 0.1047 - val_accuracy: 0.9682\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0418 - accuracy: 0.9823 - val_loss: 0.0529 - val_accuracy: 0.9773\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9682\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.9828 - val_loss: 0.0388 - val_accuracy: 0.9864\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9879 - val_loss: 0.0586 - val_accuracy: 0.9818\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9813 - val_loss: 0.0360 - val_accuracy: 0.9864\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9879 - val_loss: 0.0562 - val_accuracy: 0.9636\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0419 - accuracy: 0.9854 - val_loss: 0.0460 - val_accuracy: 0.9818\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9808 - val_loss: 0.0606 - val_accuracy: 0.9727\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9859 - val_loss: 0.1123 - val_accuracy: 0.9455\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9864 - val_loss: 0.1751 - val_accuracy: 0.9364\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9859 - val_loss: 0.0598 - val_accuracy: 0.9682\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9859 - val_loss: 0.0558 - val_accuracy: 0.9818\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9843 - val_loss: 0.0472 - val_accuracy: 0.9727\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9843 - val_loss: 0.0441 - val_accuracy: 0.9864\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9874 - val_loss: 0.0573 - val_accuracy: 0.9682\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9828 - val_loss: 0.0802 - val_accuracy: 0.9682\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9838 - val_loss: 0.0340 - val_accuracy: 0.9864\n",
      "Epoch 87/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.9854 - val_loss: 0.0671 - val_accuracy: 0.9636\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.9854 - val_loss: 0.0700 - val_accuracy: 0.9727\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9884 - val_loss: 0.0441 - val_accuracy: 0.9909\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0369 - accuracy: 0.9843 - val_loss: 0.0611 - val_accuracy: 0.9773\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9843 - val_loss: 0.0760 - val_accuracy: 0.9636\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9884 - val_loss: 0.1513 - val_accuracy: 0.9455\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9869 - val_loss: 0.1321 - val_accuracy: 0.9591\n",
      "Epoch 94/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9843 - val_loss: 0.0322 - val_accuracy: 0.9818\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9879 - val_loss: 0.0862 - val_accuracy: 0.9636\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9843 - val_loss: 0.0530 - val_accuracy: 0.9727\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.9879 - val_loss: 0.1967 - val_accuracy: 0.9364\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0355 - accuracy: 0.9869 - val_loss: 0.0325 - val_accuracy: 0.9818\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.9874 - val_loss: 0.1413 - val_accuracy: 0.9409\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9899 - val_loss: 0.0735 - val_accuracy: 0.9682\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9909 - val_loss: 0.0860 - val_accuracy: 0.9682\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0329 - accuracy: 0.9879 - val_loss: 0.0766 - val_accuracy: 0.9636\n",
      "Epoch 103/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 0.9889 - val_loss: 0.0392 - val_accuracy: 0.9773\n",
      "Epoch 104/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.0322 - val_accuracy: 0.9909\n",
      "Epoch 105/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9884 - val_loss: 0.1671 - val_accuracy: 0.9409\n",
      "Epoch 106/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0320 - accuracy: 0.9899 - val_loss: 0.0933 - val_accuracy: 0.9591\n",
      "Epoch 107/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9869 - val_loss: 0.0482 - val_accuracy: 0.9773\n",
      "Epoch 108/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9854 - val_loss: 0.0518 - val_accuracy: 0.9818\n",
      "Epoch 109/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9894 - val_loss: 0.0388 - val_accuracy: 0.9818\n",
      "Epoch 110/200\n",
      " 1/50 [..............................] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Target reached 99.00%. Stop Training\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0245 - accuracy: 0.9904 - val_loss: 0.0324 - val_accuracy: 0.9909\n",
      "Epoch 111/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0259 - accuracy: 0.9924 - val_loss: 0.0811 - val_accuracy: 0.9682\n",
      "Epoch 112/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9884 - val_loss: 0.0914 - val_accuracy: 0.9727\n",
      "Epoch 113/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.9889 - val_loss: 0.0787 - val_accuracy: 0.9682\n",
      "Epoch 114/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.9909 - val_loss: 0.0467 - val_accuracy: 0.9773\n",
      "Epoch 115/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0323 - accuracy: 0.9869 - val_loss: 0.0272 - val_accuracy: 0.9818\n",
      "Epoch 116/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.9854 - val_loss: 0.0415 - val_accuracy: 0.9773\n",
      "Epoch 117/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.9879 - val_loss: 0.0323 - val_accuracy: 0.9818\n",
      "Epoch 118/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0287 - accuracy: 0.9899 - val_loss: 0.0305 - val_accuracy: 0.9909\n",
      "Epoch 119/200\n",
      "42/50 [========================>.....] - ETA: 0s - loss: 0.0264 - accuracy: 0.9911\n",
      "Target reached 99.00%. Stop Training\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9914 - val_loss: 0.0162 - val_accuracy: 0.9955\n",
      "Epoch 120/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9889 - val_loss: 0.0580 - val_accuracy: 0.9727\n",
      "Epoch 121/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9889 - val_loss: 0.0410 - val_accuracy: 0.9773\n",
      "Epoch 122/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9889 - val_loss: 0.0743 - val_accuracy: 0.9682\n",
      "Epoch 123/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0266 - accuracy: 0.9894 - val_loss: 0.0303 - val_accuracy: 0.9864\n",
      "Epoch 124/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9894 - val_loss: 0.0619 - val_accuracy: 0.9773\n",
      "Epoch 125/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9894 - val_loss: 0.0274 - val_accuracy: 0.9955\n",
      "Epoch 126/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 0.9899 - val_loss: 0.0437 - val_accuracy: 0.9773\n",
      "Epoch 127/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0317 - accuracy: 0.9869 - val_loss: 0.0261 - val_accuracy: 0.9864\n",
      "Epoch 128/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9919 - val_loss: 0.0551 - val_accuracy: 0.9682\n",
      "Epoch 129/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9889 - val_loss: 0.0327 - val_accuracy: 0.9864\n",
      "Epoch 130/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9894 - val_loss: 0.0715 - val_accuracy: 0.9727\n",
      "Epoch 131/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9889 - val_loss: 0.0429 - val_accuracy: 0.9864\n",
      "Epoch 132/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9919 - val_loss: 0.1202 - val_accuracy: 0.9545\n",
      "Epoch 133/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.9899 - val_loss: 0.0704 - val_accuracy: 0.9727\n",
      "Epoch 134/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0302 - accuracy: 0.9904 - val_loss: 0.1020 - val_accuracy: 0.9682\n",
      "Epoch 135/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0258 - accuracy: 0.9879 - val_loss: 0.0477 - val_accuracy: 0.9818\n",
      "Epoch 136/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0271 - accuracy: 0.9909 - val_loss: 0.0698 - val_accuracy: 0.9727\n",
      "Epoch 137/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.9889 - val_loss: 0.0604 - val_accuracy: 0.9773\n",
      "Epoch 138/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9889 - val_loss: 0.0753 - val_accuracy: 0.9727\n",
      "Epoch 139/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.9884 - val_loss: 0.1800 - val_accuracy: 0.9545\n",
      "Epoch 140/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9894 - val_loss: 0.0474 - val_accuracy: 0.9773\n",
      "Epoch 141/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9919 - val_loss: 0.0403 - val_accuracy: 0.9818\n",
      "Epoch 142/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9879 - val_loss: 0.0155 - val_accuracy: 0.9955\n",
      "Epoch 143/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9899 - val_loss: 0.0450 - val_accuracy: 0.9818\n",
      "Epoch 144/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0310 - accuracy: 0.9869 - val_loss: 0.0390 - val_accuracy: 0.9818\n",
      "Epoch 145/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0258 - accuracy: 0.9899 - val_loss: 0.0327 - val_accuracy: 0.9864\n",
      "Epoch 146/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.9909 - val_loss: 0.0409 - val_accuracy: 0.9818\n",
      "Epoch 147/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9904 - val_loss: 0.0245 - val_accuracy: 0.9864\n",
      "Epoch 148/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9914 - val_loss: 0.1364 - val_accuracy: 0.9409\n",
      "Epoch 149/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9889 - val_loss: 0.0465 - val_accuracy: 0.9818\n",
      "Epoch 150/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9909 - val_loss: 0.0420 - val_accuracy: 0.9773\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.9914 - val_loss: 0.0575 - val_accuracy: 0.9727\n",
      "Epoch 152/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9879 - val_loss: 0.0511 - val_accuracy: 0.9773\n",
      "Epoch 153/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9909 - val_loss: 0.0286 - val_accuracy: 0.9864\n",
      "Epoch 154/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9899 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0277 - accuracy: 0.9894 - val_loss: 0.0960 - val_accuracy: 0.9682\n",
      "Epoch 156/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9889 - val_loss: 0.0645 - val_accuracy: 0.9727\n",
      "Epoch 157/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9894 - val_loss: 0.0959 - val_accuracy: 0.9636\n",
      "Epoch 158/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0667 - val_accuracy: 0.9727\n",
      "Epoch 159/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.9919 - val_loss: 0.0877 - val_accuracy: 0.9727\n",
      "Epoch 160/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.9864 - val_loss: 0.0941 - val_accuracy: 0.9636\n",
      "Epoch 161/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.1812 - val_accuracy: 0.9455\n",
      "Epoch 162/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9879 - val_loss: 0.0212 - val_accuracy: 0.9909\n",
      "Epoch 163/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9909 - val_loss: 0.0438 - val_accuracy: 0.9818\n",
      "Epoch 164/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9899 - val_loss: 0.0259 - val_accuracy: 0.9909\n",
      "Epoch 165/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.9879 - val_loss: 0.0442 - val_accuracy: 0.9818\n",
      "Epoch 166/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9919 - val_loss: 0.1005 - val_accuracy: 0.9636\n",
      "Epoch 167/200\n",
      "42/50 [========================>.....] - ETA: 0s - loss: 0.0224 - accuracy: 0.9917\n",
      "Target reached 99.00%. Stop Training\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9919 - val_loss: 0.0207 - val_accuracy: 0.9955\n",
      "Epoch 168/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.9889 - val_loss: 0.0312 - val_accuracy: 0.9864\n",
      "Epoch 169/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.9914 - val_loss: 0.0279 - val_accuracy: 0.9773\n",
      "Epoch 170/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.0276 - val_accuracy: 0.9864\n",
      "Epoch 171/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9909 - val_loss: 0.0262 - val_accuracy: 0.9864\n",
      "Epoch 172/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9889 - val_loss: 0.0369 - val_accuracy: 0.9773\n",
      "Epoch 173/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.9919 - val_loss: 0.0322 - val_accuracy: 0.9818\n",
      "Epoch 174/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9894 - val_loss: 0.0268 - val_accuracy: 0.9864\n",
      "Epoch 175/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9919 - val_loss: 0.0875 - val_accuracy: 0.9727\n",
      "Epoch 176/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9919 - val_loss: 0.0407 - val_accuracy: 0.9818\n",
      "Epoch 177/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9909 - val_loss: 0.0648 - val_accuracy: 0.9682\n",
      "Epoch 178/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.9909 - val_loss: 0.0729 - val_accuracy: 0.9682\n",
      "Epoch 179/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9894 - val_loss: 0.0184 - val_accuracy: 0.9909\n",
      "Epoch 180/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0166 - accuracy: 0.9934 - val_loss: 0.0451 - val_accuracy: 0.9864\n",
      "Epoch 181/200\n",
      " 1/50 [..............................] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
      "Target reached 99.00%. Stop Training\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9904 - val_loss: 0.0249 - val_accuracy: 0.9909\n",
      "Epoch 182/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9914 - val_loss: 0.0460 - val_accuracy: 0.9818\n",
      "Epoch 183/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 0.9914 - val_loss: 0.0317 - val_accuracy: 0.9864\n",
      "Epoch 184/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0186 - accuracy: 0.9937\n",
      "Target reached 99.00%. Stop Training\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0255 - val_accuracy: 0.9955\n",
      "Epoch 185/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9913\n",
      "Target reached 99.00%. Stop Training\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.9914 - val_loss: 0.0221 - val_accuracy: 0.9955\n",
      "Epoch 186/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9929 - val_loss: 0.0420 - val_accuracy: 0.9864\n",
      "Epoch 187/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0209 - accuracy: 0.9904\n",
      "Target reached 99.00%. Stop Training\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.9909 - val_loss: 0.0173 - val_accuracy: 0.9909\n",
      "Epoch 188/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9914 - val_loss: 0.0362 - val_accuracy: 0.9818\n",
      "Epoch 189/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 0.9924 - val_loss: 0.0750 - val_accuracy: 0.9773\n",
      "Epoch 190/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.0375 - val_accuracy: 0.9864\n",
      "Epoch 191/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.9919 - val_loss: 0.0657 - val_accuracy: 0.9818\n",
      "Epoch 192/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9899 - val_loss: 0.0621 - val_accuracy: 0.9818\n",
      "Epoch 193/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9904 - val_loss: 0.0280 - val_accuracy: 0.9864\n",
      "Epoch 194/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9884 - val_loss: 0.0233 - val_accuracy: 0.9955\n",
      "Epoch 195/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9914 - val_loss: 0.0313 - val_accuracy: 0.9864\n",
      "Epoch 196/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9929 - val_loss: 0.0613 - val_accuracy: 0.9773\n",
      "Epoch 197/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9909 - val_loss: 0.1026 - val_accuracy: 0.9682\n",
      "Epoch 198/200\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.0183 - accuracy: 0.9919 - val_loss: 0.0476 - val_accuracy: 0.9818\n",
      "Epoch 199/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9904 - val_loss: 0.0676 - val_accuracy: 0.9682\n",
      "Epoch 200/200\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9914 - val_loss: 0.0306 - val_accuracy: 0.9864\n"
     ]
    }
   ],
   "source": [
    "#modelling process\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(22, activation='softmax')\n",
    "])\n",
    "\n",
    "class Reminder(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ACCURACY_THRESHOLD = 0.99\n",
    "        if(logs.get('val_accuracy') > ACCURACY_THRESHOLD and logs.get('accuracy') > ACCURACY_THRESHOLD):   \n",
    "            print(\"\\nTarget reached %2.2f%%. Stop Training\" %(ACCURACY_THRESHOLD*100))\n",
    "            # self.model.stop_training = True\n",
    "\n",
    "reminderCB = Reminder()\n",
    "\n",
    "checkpoint_filepath = 'model/weights.{epoch:02d}-{val_loss:.3f}.h5'\n",
    "checkpointCB = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=checkpoint_filepath,\n",
    "                    monitor='val_accuracy',\n",
    "                    mode='max',\n",
    "                    save_best_only=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop' ,metrics=['accuracy'])\n",
    "\n",
    "fitting_history = model.fit(\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        epochs=200,\n",
    "                        steps_per_epoch = 50,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks = [reminderCB,checkpointCB],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The evaluation process is the last process in this document. This evaluation process aims to evaluate how the model works, and how the model converges through each epoch. There are to graphs going to be made, the accuracy plot, and the loss plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Plotting\n",
    "\n",
    "Plot the accuracy of the model in the epoch. The plot is for accuracy of both the train set and the test set. After plotting, check the graph. Does the accuracy continue to increase or keep decreasing? If it continues to increase, then the model is well-fit.\n",
    "\n",
    "In this case, the graph shows that the accuracy is increasing in both train and test set. This means that the model is well-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fitting_history.history['accuracy'])\n",
    "plt.plot(fitting_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plotting\n",
    "\n",
    "Plot the loss of the model in the epoch. As the opposite of accuracy, the plot is for loss of both the train set and the test set should decrease in each epoch. After plotting, check the graph. Does the loss continue to decrease or keep increasing? If it continues to decrease, then the model is well-fit.\n",
    "\n",
    "In this case, the graph shows that the loss is decreasing in both train and test set. This means that the model is well-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fitting_history.history['loss'])\n",
    "plt.plot(fitting_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the evaluation matrix (from the graph of the accuracy and the loss function), the model made was well-fit with the accuracy and the val_accuracy higher than 99.00%.\n",
    "So, we can use the best model generated from this model to predict our data."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75fe66cc5c562c8b7ab53726264ba1dcd972787fdbff729afe5074f433741fc8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
